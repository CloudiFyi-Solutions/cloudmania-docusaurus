"use strict";(self.webpackChunkcloudmania=self.webpackChunkcloudmania||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"migrate-private-pypi-packages","metadata":{"permalink":"/blog/migrate-private-pypi-packages","editUrl":"https://github.com/CloudiFyi-Solutions/tree/main/packages/create-docusaurus/templates/shared/blog/2025-05-02/index.md","source":"@site/blog/2025-05-02/index.md","title":"Migrating Private PyPI Packages Across Azure DevOps Organizations","description":"Introduction","date":"2025-05-02T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python programming language"},{"inline":false,"label":"Azure DevOps","permalink":"/blog/tags/azure-devops","description":"Azure DevOps pipelines, repos, and artifacts"},{"inline":false,"label":"PyPI","permalink":"/blog/tags/pypi","description":"Python Package Index and private package feeds"},{"inline":false,"label":"Package Management","permalink":"/blog/tags/package-management","description":"Managing and distributing software packages"},{"inline":false,"label":"DevOps","permalink":"/blog/tags/devops","description":"devops"}],"readingTime":4.17,"hasTruncateMarker":true,"authors":[{"name":"Umar Khan","title":"DevSecOps / Cloud / Platform Engineer","url":"https://github.com/ukhan262","page":{"permalink":"/blog/authors/ukhan"},"socials":{"x":"https://x.com/ukhan262","github":"https://github.com/ukhan262"},"imageURL":"https://github.com/ukhan262.png","key":"ukhan"}],"frontMatter":{"slug":"migrate-private-pypi-packages","title":"Migrating Private PyPI Packages Across Azure DevOps Organizations","authors":["ukhan"],"tags":["python","azure-devops","pypi","package-management","devops"]},"unlisted":false,"nextItem":{"title":"Reusable Terraform Deployment Workflow","permalink":"/blog/reusable-terraform-deployment"}},"content":"![](./image.png)\\n\\n## Introduction\\nMigrating private PyPI packages between Azure DevOps organizations may sound like a trivial task\u2014until you realize you don\u2019t have access to the source organization or the upstream feed.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this article, I walk through how I moved packages from a private PyPI feed in one Azure DevOps organization to another, without needing any backend or admin access to the source organization.\\n\\n## The Problem\\n\\nOur organization had multiple Azure DevOps orgs, each managing its own feeds for private Python packages. As part of an enterprise-wide initiative to centralize DevOps operations, we had to merge these orgs and move all internal packages into a unified feed.\\n\\nUnfortunately, this wasn\u2019t just a matter of updating setup.py and republishing. In many cases:\\n\\n- The source repositories no longer existed.\\n- The build pipelines had been decommissioned.\\n- The only remaining evidence of the package was its artifact in the original feed.\\n\\n## The Goal\\nBuild a CLI tool that:\\n\\n- Migrates PyPI packages across Azure DevOps orgs or projects\\n- Works even if the original source code is gone\\n- Handles .whl and .tar.gz packages\\n- Skips already uploaded or deleted versions\\n -Is resilient, idempotent, and easy to use\\n\\n## The Solution\\n\\nI built a CLI in Python that:\\n\\n- Lists available package versions from the source feed (using HTML scraping and Azure DevOps REST APIs)\\n- Downloads each package file using HTTP Basic Auth (via Personal Access Token)\\n- Uploads to the destination feed using twine, while handling common errors like duplicates or deleted versions\\n- It supports both organization-scoped and project-scoped feeds, and it can be run repeatedly without breaking anything.\\n\\n## Source Code \\n\\nSave the source code as migrate_pypi_cli.py\\n\\n```python\\nimport argparse\\nimport os\\nimport re\\nimport requests\\nimport subprocess\\nfrom pathlib import Path\\nfrom bs4 import BeautifulSoup\\n\\nUSER = \\"user\\"\\nDOWNLOAD_DIR = Path(\\"downloaded_packages\\")\\n\\n\\ndef build_urls(org, feed, project, package):\\n    base_path = f\\"{org}/{project}\\" if project else org\\n    simple_url = f\\"https://pkgs.dev.azure.com/{base_path}/_packaging/{feed}/pypi/simple/\\"\\n    upload_url = f\\"https://pkgs.dev.azure.com/{base_path}/_packaging/{feed}/pypi/upload\\"\\n    api_url = f\\"https://feeds.dev.azure.com/{base_path}/_apis/packaging/feeds/{feed}/pypi/packages/{package}/versions?api-version=7.1-preview.1\\"\\n    return simple_url, upload_url, api_url\\n\\n\\ndef get_versions(package, index_url, auth):\\n    print(f\\"\ud83d\udd0d Fetching versions for {package} from {index_url}...\\")\\n    r = requests.get(f\\"{index_url}{package}/\\", auth=auth)\\n    if r.status_code != 200:\\n        raise Exception(f\\"\u274c Failed to fetch index: {r.status_code} - {r.text}\\")\\n\\n    soup = BeautifulSoup(r.text, \\"html.parser\\")\\n    files = {}\\n    for tag in soup.find_all(\\"a\\"):\\n        href = tag.get(\\"href\\", \\"\\").split(\\"#\\")[0]\\n        filename = href.split(\\"/\\")[-1]\\n        match = re.search(rf\\"{package.replace(\'-\', \'_\')}-([0-9a-zA-Z.\\\\-_]+)\\\\.(whl|tar\\\\.gz)\\", filename)\\n        if match:\\n            version = match.group(1)\\n            files[version] = href\\n    print(f\\"\u2705 Found {len(files)} versions: {sorted(files.keys())}\\")\\n    return files\\n\\n\\ndef download_file(url, dest_path, auth):\\n    r = requests.get(url, auth=auth)\\n    if r.status_code == 200:\\n        with open(dest_path, \\"wb\\") as f:\\n            f.write(r.content)\\n        print(f\\"\u2705 Downloaded: {dest_path.name}\\")\\n    else:\\n        raise Exception(f\\"\u274c Failed to download {url} - {r.status_code}\\\\n{r.text}\\")\\n\\n\\ndef destination_version_exists(version, api_url, auth, uploaded_versions):\\n    if version in uploaded_versions:\\n        return True\\n    r = requests.get(api_url, auth=auth)\\n    if r.status_code == 200:\\n        existing = {v[\\"version\\"] for v in r.json().get(\\"value\\", [])}\\n        uploaded_versions.update(existing)\\n        return version in existing\\n    return False\\n\\n\\ndef upload_with_twine(filepath, version, upload_url, auth_password, api_url, uploaded_versions):\\n    if destination_version_exists(version, api_url, auth_password, uploaded_versions):\\n        print(f\\"\u26a0\ufe0f Skipping {version}: already exists.\\")\\n        return\\n\\n    print(f\\"\u2b06\ufe0f Uploading {filepath.name}...\\")\\n    cmd = [\\n        \\"twine\\", \\"upload\\",\\n        \\"--repository-url\\", upload_url,\\n        \\"-u\\", USER,\\n        \\"-p\\", auth_password,\\n        str(filepath)\\n    ]\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n    stderr_lower = result.stderr.lower()\\n    if result.returncode == 0 or \\"already contains file\\" in result.stdout.lower():\\n        print(f\\"\u2705 Uploaded or already exists: {filepath.name}\\")\\n        uploaded_versions.add(version)\\n    elif \\"has been deleted\\" in stderr_lower and \\"cannot be restored\\" in stderr_lower:\\n        print(f\\"\u26a0\ufe0f Skipped {filepath.name}: deleted and cannot be re-pushed.\\")\\n    else:\\n        print(f\\"\u274c Failed to upload {filepath.name}\\\\n{result.stdout}\\\\n{result.stderr}\\")\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\"Migrate PyPI packages between Azure DevOps feeds.\\")\\n    parser.add_argument(\\"--source-org\\", required=True)\\n    parser.add_argument(\\"--source-feed\\", required=True)\\n    parser.add_argument(\\"--source-pat\\", required=True)\\n    parser.add_argument(\\"--package\\", required=True)\\n    parser.add_argument(\\"--dest-org\\", required=True)\\n    parser.add_argument(\\"--dest-feed\\", required=True)\\n    parser.add_argument(\\"--dest-pat\\", required=True)\\n    parser.add_argument(\\"--source-project\\", default=\\"\\")\\n    parser.add_argument(\\"--dest-project\\", default=\\"\\")\\n    args = parser.parse_args()\\n\\n    source_index, _, _ = build_urls(args.source_org, args.source_feed, args.source_project, args.package)\\n    _, dest_upload, dest_api = build_urls(args.dest_org, args.dest_feed, args.dest_project, args.package)\\n\\n    package_dir = DOWNLOAD_DIR / args.package\\n    package_dir.mkdir(parents=True, exist_ok=True)\\n\\n    uploaded_versions = set()\\n    file_map = get_versions(args.package, source_index, auth=(USER, args.source_pat))\\n\\n    for version, url in file_map.items():\\n        filename = url.split(\\"/\\")[-1]\\n        dest_file = package_dir / filename\\n        if not dest_file.exists():\\n            try:\\n                download_file(url, dest_file, auth=(USER, args.source_pat))\\n            except Exception as e:\\n                print(f\\"\u26d4 Skipping {version}: {e}\\")\\n                continue\\n\\n    for wheel in package_dir.glob(\\"*\\"):\\n        match = re.search(rf\\"{args.package.replace(\'-\', \'_\')}-([0-9a-zA-Z.\\\\-_]+)\\\\.(whl|tar\\\\.gz)\\", wheel.name)\\n        if not match:\\n            print(f\\"\u26a0\ufe0f Skipping unrecognized file: {wheel.name}\\")\\n            continue\\n        version = match.group(1)\\n        try:\\n            upload_with_twine(wheel, version, dest_upload, args.dest_pat, dest_api, uploaded_versions)\\n        except Exception as e:\\n            print(f\\"\u26d4 Upload error for {wheel.name}: {e}\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\n## How it Works?\\n\\n```python\\n# Project Scoped\\npython migrate_pypi_cli.py \\\\\\n  --source-org \\"SourceOrg\\" \\\\\\n  --source-feed ProjectScopedFeed \\\\\\n  --source-pat $AZURE_PAT \\\\\\n  --package python-utilities-tfc \\\\\\n  --dest-org \\"DestinationOrg\\" \\\\\\n  --dest-feed ProjectScopedFeed \\\\\\n  --dest-pat $DEST_PAT \\\\\\n  --source-project Automation \\\\\\n  --dest-project Automation\\n\\n# Org Scoped\\npython migrate_pypi_cli.py \\\\\\n  --source-org \\"SourceOrg\\" \\\\\\n  --source-feed \\"SourceOrg\\" \\\\\\n  --source-pat $AZURE_PAT \\\\\\n  --package python-utilities-tfc \\\\\\n  --dest-org \\"DestinationOrg\\" \\\\\\n  --dest-feed \\"DestinationOrg \\\\\\n  --dest-pat $DEST_PAT \\\\\\n```\\n\\nThe tool:\\n\\n- Authenticates to both feeds using your PAT\\n- Downloads any .whl or .tar.gz that isn\u2019t already present\\n- Uploads each to the destination feed using twine\\n- Skips anything that already exists or was deleted\\n\\n## Expected Output\\n\\n```\\n\ud83d\udd0d Fetching versions for python-utilities-tfc from https://pkgs.dev.azure.com/...\\n\u2705 Found 2 versions: [\'0.5.1\', \'1.9.0\']\\n\\n\u2b07\ufe0f Downloading: python_utilities_tfc-0.5.1-py3-none-any.whl\\n\u2705 Downloaded: python_utilities_tfc-0.5.1-py3-none-any.whl\\n\\n\u2b06\ufe0f Uploading: python_utilities_tfc-0.5.1-py3-none-any.whl\\n\u26a0\ufe0f Skipping upload: version 0.5.1 already exists in destination.\\n\\n\u2b06\ufe0f Uploading: python_utilities_tfc-1.9.0-py3-none-any.whl\\n\u2705 Successfully uploaded: python_utilities_tfc-1.9.0-py3-none-any.whl\\n```"},{"id":"reusable-terraform-deployment","metadata":{"permalink":"/blog/reusable-terraform-deployment","editUrl":"https://github.com/CloudiFyi-Solutions/tree/main/packages/create-docusaurus/templates/shared/blog/2023-08-30/index.md","source":"@site/blog/2023-08-30/index.md","title":"Reusable Terraform Deployment Workflow","description":"In modern cloud-native development, infrastructure automation plays a pivotal role in enabling teams to deliver faster and more reliably. Terraform, being one of the leading Infrastructure as Code (IaC) tools, is frequently integrated into CI/CD pipelines to provision and manage cloud resources.","date":"2023-08-30T00:00:00.000Z","tags":[{"inline":false,"label":"Terraform","permalink":"/blog/tags/terraform","description":"Terraform"},{"inline":false,"label":"DevOps","permalink":"/blog/tags/devops","description":"devops"},{"inline":false,"label":"ci/cd","permalink":"/blog/tags/cicd","description":"Continuous Integration and Continuous Delivery"},{"inline":false,"label":"Azure","permalink":"/blog/tags/azure","description":"Microsoft Azure Cloud Platform"},{"inline":false,"label":"Automation","permalink":"/blog/tags/automation","description":"automation"}],"readingTime":1.595,"hasTruncateMarker":true,"authors":[{"name":"Umar Khan","title":"DevSecOps / Cloud / Platform Engineer","url":"https://github.com/ukhan262","page":{"permalink":"/blog/authors/ukhan"},"socials":{"x":"https://x.com/ukhan262","github":"https://github.com/ukhan262"},"imageURL":"https://github.com/ukhan262.png","key":"ukhan"}],"frontMatter":{"slug":"reusable-terraform-deployment","title":"Reusable Terraform Deployment Workflow","authors":["ukhan"],"tags":["terraform","devops","ci/cd","azure","automation"]},"unlisted":false,"prevItem":{"title":"Migrating Private PyPI Packages Across Azure DevOps Organizations","permalink":"/blog/migrate-private-pypi-packages"}},"content":"\x3c!-- truncate --\x3e\\n\\nIn modern cloud-native development, infrastructure automation plays a pivotal role in enabling teams to deliver faster and more reliably. Terraform, being one of the leading Infrastructure as Code (IaC) tools, is frequently integrated into CI/CD pipelines to provision and manage cloud resources.\\n\\n![](./visuals.png)\\n\\n\\nThis article discusses how to create a reusable Terraform deployment workflow that can be easily integrated into any DevOps pipeline, especially within platforms like Azure DevOps or GitHub Actions.\\n\\n## Why Reusable Workflows?\\n\\nReusable workflows promote consistency, reduce duplication, and make it easier to manage infrastructure across different environments or projects. Instead of copying and pasting Terraform deployment logic in every pipeline, you can define it once and reuse it.\\n\\n## Workflow Design\\n\\nAt a high level, a Terraform deployment workflow consists of the following stages:\\n\\n1. **Initialization**  \\n   Run `terraform init` to initialize the working directory containing Terraform configuration files.\\n\\n2. **Validation**  \\n   Use `terraform validate` to check whether the configuration is syntactically valid and internally consistent.\\n\\n3. **Formatting**  \\n   `terraform fmt -check` ensures that the code is formatted according to best practices.\\n\\n4. **Planning**  \\n   Execute `terraform plan` to see the changes Terraform will apply.\\n\\n5. **Apply (optional)**  \\n   `terraform apply` to make actual changes to the infrastructure, gated by approvals or specific conditions.\\n\\n## Modular Pipeline Example\\n\\nYou can create a reusable pipeline template that accepts variables like:\\n\\n- Path to Terraform code\\n- Target environment\\n- Resource group or backend config\\n\\nThis allows the same pipeline logic to be applied across multiple modules or environments.\\n\\n## Benefits\\n\\n- **Scalability**: Easily apply the same logic across teams.\\n- **Security**: Enforce approval gates before applying.\\n- **Audibility**: Centralized logs and visibility of deployment.\\n\\n## Conclusion\\n\\nBy building reusable Terraform workflows, organizations can accelerate their infrastructure delivery while reducing operational overhead. This approach also helps enforce governance and best practices consistently.\\n\\n> For a more detailed walkthrough and pipeline YAML examples, check the full article on [Medium](https://medium.com/@umarkhan_87391/reusable-terraform-deployment-workflow-bdf35b8529f4)."}]}}')}}]);